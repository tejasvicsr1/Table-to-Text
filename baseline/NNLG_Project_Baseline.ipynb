{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXpxatXNkr_p",
        "outputId": "2652712d-0bb0-45b0-86b0-ccfc99bcd6a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â¬ Downloading https://github.com/jaimergp/miniforge/releases/latest/download/Mambaforge-colab-Linux-x86_64.sh...\n",
            "ğŸ“¦ Installing...\n",
            "ğŸ“Œ Adjusting configuration...\n",
            "ğŸ©¹ Patching environment...\n",
            "â² Done in 0:00:21\n",
            "ğŸ” Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "!pip install -q condacolab\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/RUCAIBox/TextBox.git"
      ],
      "metadata": {
        "id": "rHlqFoCF013q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd TextBox && bash install.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSrREh76vPfd",
        "outputId": "532febce-8ffb-4e3a-da74-b578f13a5f9e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—  â–ˆâ–ˆâ•—\n",
            "â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â•â•â•â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•\n",
            "   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—   â•šâ–ˆâ–ˆâ–ˆâ•”â•    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ•”â•\n",
            "   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â•   â–ˆâ–ˆâ•”â–ˆâ–ˆâ•—    â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘ â–ˆâ–ˆâ•”â–ˆâ–ˆâ•—\n",
            "   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•—\n",
            "   â•šâ•â•   â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•   â•šâ•â•   â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•\n",
            "\n",
            "A modified version of transformers will be installed to python environment. Create a new conda environment (TextBox)? (y/n) y\n",
            "Creating conda environment named TextBox (python=3.8) ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 22.9.0\n",
            "  latest version: 22.11.1\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c conda-forge conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local/envs/TextBox\n",
            "\n",
            "  added / updated specs:\n",
            "    - python=3.8\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    ca-certificates-2022.12.7  |       ha878542_0         143 KB  conda-forge\n",
            "    expat-2.5.0                |       h27087fc_0         189 KB  conda-forge\n",
            "    gdbm-1.18                  |       h0a1914f_2         190 KB  conda-forge\n",
            "    openssl-3.0.7              |       h0b41bf4_2         2.5 MB  conda-forge\n",
            "    pip-23.0                   |     pyhd8ed1ab_0         1.3 MB  conda-forge\n",
            "    pypy3.8-7.3.11             |       h75e580f_0        31.3 MB  conda-forge\n",
            "    python-3.8.16              |        0_73_pypy           7 KB  conda-forge\n",
            "    python_abi-3.8             |    3_pypy38_pp73           6 KB  conda-forge\n",
            "    setuptools-66.1.1          |     pyhd8ed1ab_0         630 KB  conda-forge\n",
            "    sqlite-3.40.0              |       h4ff8645_0         801 KB  conda-forge\n",
            "    zlib-1.2.13                |       h166bdaf_4          92 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        37.1 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      conda-forge/linux-64::_libgcc_mutex-0.1-conda_forge None\n",
            "  _openmp_mutex      conda-forge/linux-64::_openmp_mutex-4.5-2_gnu None\n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h7f98852_4 None\n",
            "  ca-certificates    conda-forge/linux-64::ca-certificates-2022.12.7-ha878542_0 None\n",
            "  expat              conda-forge/linux-64::expat-2.5.0-h27087fc_0 None\n",
            "  gdbm               conda-forge/linux-64::gdbm-1.18-h0a1914f_2 None\n",
            "  libffi             conda-forge/linux-64::libffi-3.4.2-h7f98852_5 None\n",
            "  libgcc-ng          conda-forge/linux-64::libgcc-ng-12.2.0-h65d4601_19 None\n",
            "  libgomp            conda-forge/linux-64::libgomp-12.2.0-h65d4601_19 None\n",
            "  libsqlite          conda-forge/linux-64::libsqlite-3.40.0-h753d276_0 None\n",
            "  libstdcxx-ng       conda-forge/linux-64::libstdcxx-ng-12.2.0-h46fd767_19 None\n",
            "  libzlib            conda-forge/linux-64::libzlib-1.2.13-h166bdaf_4 None\n",
            "  ncurses            conda-forge/linux-64::ncurses-6.3-h27087fc_1 None\n",
            "  openssl            conda-forge/linux-64::openssl-3.0.7-h0b41bf4_2 None\n",
            "  pip                conda-forge/noarch::pip-23.0-pyhd8ed1ab_0 None\n",
            "  pypy3.8            conda-forge/linux-64::pypy3.8-7.3.11-h75e580f_0 None\n",
            "  python             conda-forge/linux-64::python-3.8.16-0_73_pypy None\n",
            "  python_abi         conda-forge/linux-64::python_abi-3.8-3_pypy38_pp73 None\n",
            "  readline           conda-forge/linux-64::readline-8.1.2-h0f457ee_0 None\n",
            "  setuptools         conda-forge/noarch::setuptools-66.1.1-pyhd8ed1ab_0 None\n",
            "  sqlite             conda-forge/linux-64::sqlite-3.40.0-h4ff8645_0 None\n",
            "  tk                 conda-forge/linux-64::tk-8.6.12-h27826a3_0 None\n",
            "  wheel              conda-forge/noarch::wheel-0.38.4-pyhd8ed1ab_0 None\n",
            "  xz                 conda-forge/linux-64::xz-5.2.6-h166bdaf_0 None\n",
            "  zlib               conda-forge/linux-64::zlib-1.2.13-h166bdaf_4 None\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "pip-23.0             | 1.3 MB    | : 100% 1.0/1 [00:00<00:00,  3.92it/s]\n",
            "ca-certificates-2022 | 143 KB    | : 100% 1.0/1 [00:00<00:00, 24.17it/s]\n",
            "python-3.8.16        | 7 KB      | : 100% 1.0/1 [00:00<00:00, 29.10it/s]\n",
            "pypy3.8-7.3.11       | 31.3 MB   | : 100% 1.0/1 [00:02<00:00,  2.11s/it]               \n",
            "sqlite-3.40.0        | 801 KB    | : 100% 1.0/1 [00:00<00:00,  5.87it/s]\n",
            "openssl-3.0.7        | 2.5 MB    | : 100% 1.0/1 [00:00<00:00,  6.67it/s]\n",
            "setuptools-66.1.1    | 630 KB    | : 100% 1.0/1 [00:00<00:00,  8.63it/s]\n",
            "zlib-1.2.13          | 92 KB     | : 100% 1.0/1 [00:00<00:00, 19.27it/s]\n",
            "python_abi-3.8       | 6 KB      | : 100% 1.0/1 [00:00<00:00, 38.06it/s]\n",
            "gdbm-1.18            | 190 KB    | : 100% 1.0/1 [00:00<00:00, 14.86it/s]\n",
            "expat-2.5.0          | 189 KB    | : 100% 1.0/1 [00:00<00:00, 14.72it/s]\n",
            "Preparing transaction: \\ \b\b| \b\b/ \b\bdone\n",
            "Verifying transaction: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "#\n",
            "# To activate this environment, use\n",
            "#\n",
            "#     $ conda activate TextBox\n",
            "#\n",
            "# To deactivate an active environment, use\n",
            "#\n",
            "#     $ conda deactivate\n",
            "\n",
            "Retrieving notices: ...working... done\n",
            "\n",
            "CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\n",
            "To initialize your shell, run\n",
            "\n",
            "    $ conda init <SHELL_NAME>\n",
            "\n",
            "Currently supported shells are:\n",
            "  - bash\n",
            "  - fish\n",
            "  - tcsh\n",
            "  - xonsh\n",
            "  - zsh\n",
            "  - powershell\n",
            "\n",
            "See 'conda init --help' for more information and options.\n",
            "\n",
            "IMPORTANT: You may need to close and restart your shell after running 'conda init'.\n",
            "\n",
            "\n",
            "Installation may take a few minutes.\n",
            "\u001b[0;32mInstalling torch ...\u001b[0m\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\bfailed with initial frozen solve. Retrying with flexible solve.\n",
            "Collecting package metadata (repodata.json): \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- WARNING conda.core.solve:_add_specs(652): pinned spec cudatoolkit=11.2 conflicts with explicit specs.  Overriding pinned spec.\n",
            "\b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - cudatoolkit=11.3\n",
            "    - pytorch==1.12.1\n",
            "    - torchaudio==0.12.1\n",
            "    - torchvision==0.13.1\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _openmp_mutex-4.5          |       2_kmp_llvm           6 KB  conda-forge\n",
            "    blas-2.116                 |              mkl          13 KB  conda-forge\n",
            "    blas-devel-3.9.0           |   16_linux64_mkl          12 KB  conda-forge\n",
            "    certifi-2022.12.7          |     pyhd8ed1ab_0         147 KB  conda-forge\n",
            "    colorama-0.4.6             |     pyhd8ed1ab_0          25 KB  conda-forge\n",
            "    conda-22.11.1              |   py38h578d9bd_1         905 KB  conda-forge\n",
            "    cudatoolkit-11.3.1         |      h9edb442_11       547.6 MB  conda-forge\n",
            "    ffmpeg-4.3                 |       hf484d3e_0         9.9 MB  pytorch\n",
            "    freetype-2.12.1            |       hca18f0e_1         611 KB  conda-forge\n",
            "    gmp-6.2.1                  |       h58526e2_0         806 KB  conda-forge\n",
            "    gnutls-3.6.13              |       h85f3911_1         2.0 MB  conda-forge\n",
            "    jpeg-9e                    |       h166bdaf_2         269 KB  conda-forge\n",
            "    lame-3.100                 |    h166bdaf_1003         496 KB  conda-forge\n",
            "    lcms2-2.14                 |       hfd0df8a_1         235 KB  conda-forge\n",
            "    lerc-4.0.0                 |       h27087fc_0         275 KB  conda-forge\n",
            "    libblas-3.9.0              |   16_linux64_mkl          13 KB  conda-forge\n",
            "    libcblas-3.9.0             |   16_linux64_mkl          12 KB  conda-forge\n",
            "    libdeflate-1.17            |       h0b41bf4_0          63 KB  conda-forge\n",
            "    libgfortran-ng-12.2.0      |      h69a702a_19          22 KB  conda-forge\n",
            "    libgfortran5-12.2.0        |      h337968e_19         1.8 MB  conda-forge\n",
            "    libhwloc-2.8.0             |       h32351e8_1         3.0 MB  conda-forge\n",
            "    libjpeg-turbo-2.1.4        |       h166bdaf_0         988 KB  conda-forge\n",
            "    liblapack-3.9.0            |   16_linux64_mkl          12 KB  conda-forge\n",
            "    liblapacke-3.9.0           |   16_linux64_mkl          12 KB  conda-forge\n",
            "    libpng-1.6.39              |       h753d276_0         276 KB  conda-forge\n",
            "    libtiff-4.5.0              |       h6adf6a1_2         397 KB  conda-forge\n",
            "    libwebp-base-1.2.4         |       h166bdaf_0         404 KB  conda-forge\n",
            "    libxcb-1.13                |    h7f98852_1004         391 KB  conda-forge\n",
            "    llvm-openmp-15.0.7         |       h0cdce71_0         3.1 MB  conda-forge\n",
            "    mkl-2022.1.0               |     h84fe81f_915       199.6 MB  conda-forge\n",
            "    mkl-devel-2022.1.0         |     ha770c72_916          25 KB  conda-forge\n",
            "    mkl-include-2022.1.0       |     h84fe81f_915         745 KB  conda-forge\n",
            "    nettle-3.6                 |       he412f7d_0         6.5 MB  conda-forge\n",
            "    numpy-1.24.1               |   py38h10c12cc_0         6.4 MB  conda-forge\n",
            "    openh264-2.1.1             |       h780b84a_0         1.5 MB  conda-forge\n",
            "    openjpeg-2.5.0             |       hfec8fc6_2         344 KB  conda-forge\n",
            "    pillow-9.4.0               |   py38hb32c036_0        44.1 MB  conda-forge\n",
            "    pluggy-1.0.0               |     pyhd8ed1ab_5          16 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    h36c2ea0_1001           5 KB  conda-forge\n",
            "    pytorch-1.12.1             |py3.8_cuda11.3_cudnn8.3.2_0        1.19 GB  pytorch\n",
            "    pytorch-mutex-1.0          |             cuda           3 KB  pytorch\n",
            "    ruamel.yaml-0.17.21        |   py38h0a891b7_2         172 KB  conda-forge\n",
            "    ruamel.yaml.clib-0.2.7     |   py38h1de0b5d_1         143 KB  conda-forge\n",
            "    tbb-2021.7.0               |       h924138e_1         1.5 MB  conda-forge\n",
            "    torchaudio-0.12.1          |       py38_cu113         6.2 MB  pytorch\n",
            "    torchvision-0.13.1         |       py38_cu113        28.7 MB  pytorch\n",
            "    tqdm-4.64.1                |     pyhd8ed1ab_0          82 KB  conda-forge\n",
            "    typing_extensions-4.4.0    |     pyha770c72_0          29 KB  conda-forge\n",
            "    xorg-libxau-1.0.9          |       h7f98852_0          13 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.3        |       h7f98852_0          19 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        2.04 GB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  blas               conda-forge/linux-64::blas-2.116-mkl None\n",
            "  blas-devel         conda-forge/linux-64::blas-devel-3.9.0-16_linux64_mkl None\n",
            "  colorama           conda-forge/noarch::colorama-0.4.6-pyhd8ed1ab_0 None\n",
            "  cudatoolkit        conda-forge/linux-64::cudatoolkit-11.3.1-h9edb442_11 None\n",
            "  ffmpeg             pytorch/linux-64::ffmpeg-4.3-hf484d3e_0 None\n",
            "  freetype           conda-forge/linux-64::freetype-2.12.1-hca18f0e_1 None\n",
            "  gmp                conda-forge/linux-64::gmp-6.2.1-h58526e2_0 None\n",
            "  gnutls             conda-forge/linux-64::gnutls-3.6.13-h85f3911_1 None\n",
            "  jpeg               conda-forge/linux-64::jpeg-9e-h166bdaf_2 None\n",
            "  lame               conda-forge/linux-64::lame-3.100-h166bdaf_1003 None\n",
            "  lcms2              conda-forge/linux-64::lcms2-2.14-hfd0df8a_1 None\n",
            "  lerc               conda-forge/linux-64::lerc-4.0.0-h27087fc_0 None\n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-16_linux64_mkl None\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-16_linux64_mkl None\n",
            "  libdeflate         conda-forge/linux-64::libdeflate-1.17-h0b41bf4_0 None\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-12.2.0-h69a702a_19 None\n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-12.2.0-h337968e_19 None\n",
            "  libhwloc           conda-forge/linux-64::libhwloc-2.8.0-h32351e8_1 None\n",
            "  libjpeg-turbo      conda-forge/linux-64::libjpeg-turbo-2.1.4-h166bdaf_0 None\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-16_linux64_mkl None\n",
            "  liblapacke         conda-forge/linux-64::liblapacke-3.9.0-16_linux64_mkl None\n",
            "  libpng             conda-forge/linux-64::libpng-1.6.39-h753d276_0 None\n",
            "  libtiff            conda-forge/linux-64::libtiff-4.5.0-h6adf6a1_2 None\n",
            "  libwebp-base       conda-forge/linux-64::libwebp-base-1.2.4-h166bdaf_0 None\n",
            "  libxcb             conda-forge/linux-64::libxcb-1.13-h7f98852_1004 None\n",
            "  llvm-openmp        conda-forge/linux-64::llvm-openmp-15.0.7-h0cdce71_0 None\n",
            "  mkl                conda-forge/linux-64::mkl-2022.1.0-h84fe81f_915 None\n",
            "  mkl-devel          conda-forge/linux-64::mkl-devel-2022.1.0-ha770c72_916 None\n",
            "  mkl-include        conda-forge/linux-64::mkl-include-2022.1.0-h84fe81f_915 None\n",
            "  nettle             conda-forge/linux-64::nettle-3.6-he412f7d_0 None\n",
            "  numpy              conda-forge/linux-64::numpy-1.24.1-py38h10c12cc_0 None\n",
            "  openh264           conda-forge/linux-64::openh264-2.1.1-h780b84a_0 None\n",
            "  openjpeg           conda-forge/linux-64::openjpeg-2.5.0-hfec8fc6_2 None\n",
            "  pillow             conda-forge/linux-64::pillow-9.4.0-py38hb32c036_0 None\n",
            "  pluggy             conda-forge/noarch::pluggy-1.0.0-pyhd8ed1ab_5 None\n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h36c2ea0_1001 None\n",
            "  pytorch            pytorch/linux-64::pytorch-1.12.1-py3.8_cuda11.3_cudnn8.3.2_0 None\n",
            "  pytorch-mutex      pytorch/noarch::pytorch-mutex-1.0-cuda None\n",
            "  ruamel.yaml        conda-forge/linux-64::ruamel.yaml-0.17.21-py38h0a891b7_2 None\n",
            "  ruamel.yaml.clib   conda-forge/linux-64::ruamel.yaml.clib-0.2.7-py38h1de0b5d_1 None\n",
            "  tbb                conda-forge/linux-64::tbb-2021.7.0-h924138e_1 None\n",
            "  torchaudio         pytorch/linux-64::torchaudio-0.12.1-py38_cu113 None\n",
            "  torchvision        pytorch/linux-64::torchvision-0.13.1-py38_cu113 None\n",
            "  tqdm               conda-forge/noarch::tqdm-4.64.1-pyhd8ed1ab_0 None\n",
            "  typing_extensions  conda-forge/noarch::typing_extensions-4.4.0-pyha770c72_0 None\n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.9-h7f98852_0 None\n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h7f98852_0 None\n",
            "  zlib               conda-forge/linux-64::zlib-1.2.13-h166bdaf_4 None\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates                      2022.9.24-ha878542_0 --> 2022.12.7-ha878542_0 None\n",
            "  certifi                            2022.9.24-pyhd8ed1ab_0 --> 2022.12.7-pyhd8ed1ab_0 None\n",
            "  conda                               22.9.0-py38h578d9bd_2 --> 22.11.1-py38h578d9bd_1 None\n",
            "  openssl                                  3.0.7-h0b41bf4_1 --> 3.0.7-h0b41bf4_2 None\n",
            "\n",
            "The following packages will be DOWNGRADED:\n",
            "\n",
            "  _openmp_mutex                                   4.5-2_gnu --> 4.5-2_kmp_llvm None\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "libwebp-base-1.2.4   | 404 KB    | : 100% 1.0/1 [00:00<00:00,  6.06it/s]\n",
            "typing_extensions-4. | 29 KB     | : 100% 1.0/1 [00:00<00:00, 11.37it/s]\n",
            "libxcb-1.13          | 391 KB    | : 100% 1.0/1 [00:00<00:00,  6.40it/s]\n",
            "tbb-2021.7.0         | 1.5 MB    | : 100% 1.0/1 [00:00<00:00, 10.47it/s]\n",
            "pthread-stubs-0.4    | 5 KB      | : 100% 1.0/1 [00:00<00:00, 28.26it/s]\n",
            "certifi-2022.12.7    | 147 KB    | : 100% 1.0/1 [00:00<00:00, 24.79it/s]\n",
            "openjpeg-2.5.0       | 344 KB    | : 100% 1.0/1 [00:00<00:00, 16.67it/s]\n",
            "nettle-3.6           | 6.5 MB    | : 100% 1.0/1 [00:00<00:00,  1.03it/s]               \n",
            "libgfortran5-12.2.0  | 1.8 MB    | : 100% 1.0/1 [00:00<00:00,  3.09it/s]\n",
            "libjpeg-turbo-2.1.4  | 988 KB    | : 100% 1.0/1 [00:00<00:00,  4.84it/s]\n",
            "ruamel.yaml.clib-0.2 | 143 KB    | : 100% 1.0/1 [00:00<00:00, 21.51it/s]\n",
            "llvm-openmp-15.0.7   | 3.1 MB    | : 100% 1.0/1 [00:00<00:00,  9.39it/s]\n",
            "ffmpeg-4.3           | 9.9 MB    | : 100% 1.0/1 [00:01<00:00,  1.41s/it]               \n",
            "liblapacke-3.9.0     | 12 KB     | : 100% 1.0/1 [00:00<00:00, 22.50it/s]\n",
            "mkl-devel-2022.1.0   | 25 KB     | : 100% 1.0/1 [00:00<00:00, 21.35it/s]\n",
            "lerc-4.0.0           | 275 KB    | : 100% 1.0/1 [00:00<00:00, 12.22it/s]\n",
            "blas-2.116           | 13 KB     | : 100% 1.0/1 [00:00<00:00, 21.58it/s]\n",
            "blas-devel-3.9.0     | 12 KB     | : 100% 1.0/1 [00:00<00:00, 12.40it/s]\n",
            "freetype-2.12.1      | 611 KB    | : 100% 1.0/1 [00:00<00:00, 17.30it/s]\n",
            "_openmp_mutex-4.5    | 6 KB      | : 100% 1.0/1 [00:00<00:00, 27.89it/s]\n",
            "mkl-2022.1.0         | 199.6 MB  | : 100% 1.0/1 [00:32<00:00, 32.93s/it]              \n",
            "gnutls-3.6.13        | 2.0 MB    | : 100% 1.0/1 [00:00<00:00,  1.60it/s]\n",
            "libhwloc-2.8.0       | 3.0 MB    | : 100% 1.0/1 [00:00<00:00,  2.31it/s]\n",
            "gmp-6.2.1            | 806 KB    | : 100% 1.0/1 [00:00<00:00,  5.69it/s]\n",
            "libtiff-4.5.0        | 397 KB    | : 100% 1.0/1 [00:00<00:00, 20.06it/s]\n",
            "libcblas-3.9.0       | 12 KB     | : 100% 1.0/1 [00:00<00:00, 22.30it/s]\n",
            "liblapack-3.9.0      | 12 KB     | : 100% 1.0/1 [00:00<00:00, 16.21it/s]\n",
            "pillow-9.4.0         | 44.1 MB   | : 100% 1.0/1 [00:01<00:00,  1.38s/it]               \n",
            "ruamel.yaml-0.17.21  | 172 KB    | : 100% 1.0/1 [00:00<00:00, 11.17it/s]\n",
            "xorg-libxau-1.0.9    | 13 KB     | : 100% 1.0/1 [00:00<00:00, 24.05it/s]\n",
            "xorg-libxdmcp-1.1.3  | 19 KB     | : 100% 1.0/1 [00:00<00:00, 27.56it/s]\n",
            "libblas-3.9.0        | 13 KB     | : 100% 1.0/1 [00:00<00:00, 24.12it/s]\n",
            "cudatoolkit-11.3.1   | 547.6 MB  | : 100% 1.0/1 [00:13<00:00, 13.08s/it]               \n",
            "colorama-0.4.6       | 25 KB     | : 100% 1.0/1 [00:00<00:00, 19.15it/s]\n",
            "lcms2-2.14           | 235 KB    | : 100% 1.0/1 [00:00<00:00, 20.51it/s]\n",
            "torchaudio-0.12.1    | 6.2 MB    | : 100% 1.0/1 [00:00<00:00,  1.09it/s]\n",
            "conda-22.11.1        | 905 KB    | : 100% 1.0/1 [00:00<00:00,  4.87it/s]\n",
            "torchvision-0.13.1   | 28.7 MB   | : 100% 1.0/1 [00:03<00:00,  3.93s/it]\n",
            "tqdm-4.64.1          | 82 KB     | : 100% 1.0/1 [00:00<00:00, 16.63it/s]\n",
            "numpy-1.24.1         | 6.4 MB    | : 100% 1.0/1 [00:00<00:00,  1.49it/s]\n",
            "openh264-2.1.1       | 1.5 MB    | : 100% 1.0/1 [00:00<00:00,  3.79it/s]\n",
            "libdeflate-1.17      | 63 KB     | : 100% 1.0/1 [00:00<00:00, 23.03it/s]\n",
            "pytorch-mutex-1.0    | 3 KB      | : 100% 1.0/1 [00:00<00:00, 31.87it/s]\n",
            "pluggy-1.0.0         | 16 KB     | : 100% 1.0/1 [00:00<00:00, 28.98it/s]\n",
            "pytorch-1.12.1       | 1.19 GB   | : 100% 1.0/1 [02:41<00:00, 161.50s/it]              \n",
            "libgfortran-ng-12.2. | 22 KB     | : 100% 1.0/1 [00:00<00:00, 14.78it/s]\n",
            "jpeg-9e              | 269 KB    | : 100% 1.0/1 [00:00<00:00, 11.48it/s]\n",
            "mkl-include-2022.1.0 | 745 KB    | : 100% 1.0/1 [00:00<00:00,  2.60it/s]\n",
            "lame-3.100           | 496 KB    | : 100% 1.0/1 [00:00<00:00,  9.24it/s]\n",
            "libpng-1.6.39        | 276 KB    | : 100% 1.0/1 [00:00<00:00, 23.60it/s]\n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
            "\n",
            "\b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Retrieving notices: ...working... done\n",
            "\u001b[0;32mInstalling requirements ...\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.24.1)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.6.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (4.64.1)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (701 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m701.2/701.2 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycocoevalcap\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score>=0.1.2\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu>=2.0.0\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (1.12.1)\n",
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.12-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard\n",
            "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting future\n",
            "  Downloading future-0.18.3.tar.gz (840 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pandas\n",
            "  Downloading pandas-1.5.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.13.9-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from -r requirements.txt (line 17)) (65.5.1)\n",
            "Collecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.16.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperopt>=0.2.6\n",
            "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cycler>=0.10\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 2)) (9.4.0)\n",
            "Collecting python-dateutil>=2.7\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting contourpy>=1.0.1\n",
            "  Downloading contourpy-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m300.0/300.0 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging>=20.0\n",
            "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyparsing>=2.2.1\n",
            "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting click\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex>=2021.8.3\n",
            "  Downloading regex-2022.10.31-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (772 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m772.3/772.3 kB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycocotools>=2.0.2\n",
            "  Downloading pycocotools-2.0.6.tar.gz (24 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting absl-py\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tabulate>=0.8.9\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.8/site-packages (from sacrebleu>=2.0.0->-r requirements.txt (line 9)) (0.4.6)\n",
            "Collecting lxml\n",
            "  Downloading lxml-4.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.8/site-packages (from torch->-r requirements.txt (line 10)) (4.4.0)\n",
            "Collecting transformers>=3.0.0\n",
            "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from bert_score->-r requirements.txt (line 11)) (2.28.1)\n",
            "Collecting protobuf<4,>=3.9.2\n",
            "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio>=1.24.3\n",
            "  Downloading grpcio-1.51.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting werkzeug>=1.0.1\n",
            "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.7/232.7 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m96.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m93.3/93.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
            "  Downloading google_auth-2.16.0-py2.py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m177.8/177.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/site-packages (from tensorboard->-r requirements.txt (line 12)) (0.38.4)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting pytz>=2020.1\n",
            "  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psutil>=5.0.0\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m280.2/280.2 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting appdirs>=1.4.3\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.14.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m178.9/178.9 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m184.0/184.0 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy\n",
            "  Downloading scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting networkx>=2.2\n",
            "  Downloading networkx-3.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py4j\n",
            "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cloudpickle\n",
            "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting importlib-metadata>=4.4\n",
            "  Downloading importlib_metadata-6.0.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->bert_score->-r requirements.txt (line 11)) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->bert_score->-r requirements.txt (line 11)) (3.4)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.8/site-packages (from requests->bert_score->-r requirements.txt (line 11)) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->bert_score->-r requirements.txt (line 11)) (1.26.13)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m190.3/190.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.1.1\n",
            "  Downloading MarkupSafe-2.1.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.12.0-py3-none-any.whl (6.6 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rouge-score, future, typing, pycocotools, pathtools\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=a44c34eaba993248e373c8b49d6f5ab140a7cd4df2e39ede8553f46d1651b81d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/a8/47/66a086bcca0e429716cd68c4da6b7f9ac7976868735937eb63\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492025 sha256=fa523cb36aca57d1ffd3d6197a75c991d625879fc6a95f29b0dfc2c85c468c26\n",
            "  Stored in directory: /root/.cache/pip/wheels/a6/db/41/71a0e5d071a14e716cc11bb021a9caa8f76ec337eca071487e\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26304 sha256=c37d2563005c3099c2d69f684bc50eda5b6cfbfa611552258266fc06c8bf30e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/67/e6/f18c54806af15a742468564ece3748d4ce1adfbf9414578d77\n",
            "  Building wheel for pycocotools (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0.6-cp38-cp38-linux_x86_64.whl size=105323 sha256=2f872ca2d8e16ac438d72b30ab65c17088776cf78411cb0952363cf1033d6063\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/42/0a/26d14b3a7343223fcee09c101e73d82fff8bdd1541d85fe033\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=de3d5751cb6e61cae25270393cf9291440f79c5f1cac7d4f8c516cf22403429a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/33/74/7c0903053e955973d5dc3d21857a29b3f8c0806ad0b05c32a1\n",
            "Successfully built rouge-score future typing pycocotools pathtools\n",
            "Installing collected packages: tokenizers, tensorboard-plugin-wit, sentencepiece, pytz, pyasn1, py4j, pathtools, appdirs, zipp, typing, tensorboard-data-server, tabulate, smmap, six, setproctitle, sentry-sdk, scipy, rsa, regex, pyyaml, pyparsing, pyasn1-modules, psutil, protobuf, portalocker, packaging, oauthlib, networkx, MarkupSafe, lxml, kiwisolver, joblib, grpcio, future, fonttools, filelock, cycler, contourpy, cloudpickle, click, cachetools, absl-py, werkzeug, sacrebleu, requests-oauthlib, python-dateutil, nltk, importlib-metadata, hyperopt, huggingface-hub, google-auth, gitdb, docker-pycreds, accelerate, transformers, rouge-score, pandas, matplotlib, markdown, google-auth-oauthlib, GitPython, wandb, tensorboard, pycocotools, bert_score, pycocoevalcap\n",
            "Successfully installed GitPython-3.1.30 MarkupSafe-2.1.2 absl-py-1.4.0 accelerate-0.16.0 appdirs-1.4.4 bert_score-0.3.12 cachetools-5.3.0 click-8.1.3 cloudpickle-2.2.1 contourpy-1.0.7 cycler-0.11.0 docker-pycreds-0.4.0 filelock-3.9.0 fonttools-4.38.0 future-0.18.3 gitdb-4.0.10 google-auth-2.16.0 google-auth-oauthlib-0.4.6 grpcio-1.51.1 huggingface-hub-0.12.0 hyperopt-0.2.7 importlib-metadata-6.0.0 joblib-1.2.0 kiwisolver-1.4.4 lxml-4.9.2 markdown-3.4.1 matplotlib-3.6.3 networkx-3.0 nltk-3.8.1 oauthlib-3.2.2 packaging-23.0 pandas-1.5.3 pathtools-0.1.2 portalocker-2.7.0 protobuf-3.20.3 psutil-5.9.4 py4j-0.10.9.7 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycocoevalcap-1.2 pycocotools-2.0.6 pyparsing-3.0.9 python-dateutil-2.8.2 pytz-2022.7.1 pyyaml-6.0 regex-2022.10.31 requests-oauthlib-1.3.1 rouge-score-0.1.2 rsa-4.9 sacrebleu-2.3.1 scipy-1.10.0 sentencepiece-0.1.97 sentry-sdk-1.14.0 setproctitle-1.3.2 six-1.16.0 smmap-5.0.0 tabulate-0.9.0 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tokenizers-0.13.2 transformers-4.26.0 typing-3.7.4.3 wandb-0.13.9 werkzeug-2.2.2 zipp-3.12.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[0;32mInstalling requirements (fast-bleu) ...\u001b[0m\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[0;32mInstalling requirements (fastseq) ...\u001b[0m\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/microsoft/fastseq.git /tmp/pip-req-build-29p8eq9d\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[0;32mInstalling requirements (rouge) ...\u001b[0m\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/pltrdy/pyrouge.git /tmp/pip-req-build-6xim2st5\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCloning into 'files2rouge'...\n",
            "remote: Enumerating objects: 278, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 278 (delta 9), reused 11 (delta 6), pack-reused 258\u001b[K\n",
            "Receiving objects: 100% (278/278), 212.16 KiB | 456.00 KiB/s, done.\n",
            "Resolving deltas: 100% (133/133), done.\n",
            "/usr/local/lib/python3.8/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/site-packages/setuptools/command/easy_install.py:144: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
            "  warnings.warn(\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "files2rouge.__pycache__.settings.cpython-38: module references __file__\n",
            "rm: cannot remove '/root/.files2rouge/data/WordNet-2.0-Exceptions/WordNet-2.0.exc.db': No such file or directory\n",
            "adj.exc\n",
            "Cannot open exception file: adj.exc\n",
            "\u001b[33mWARNING: Skipping py-rouge as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[0;32mInstalling requirements (transformers) ...\u001b[0m\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 110697, done.\u001b[K\n",
            "remote: Total 110697 (delta 0), reused 0 (delta 0), pack-reused 110697\u001b[K\n",
            "Receiving objects: 100% (110697/110697), 108.00 MiB | 13.96 MiB/s, done.\n",
            "Resolving deltas: 100% (82784/82784), done.\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mW&B enabled.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbC69LvD_lr9",
        "outputId": "71fcc665-2394-4a47-9b84-b4c796406259"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar zxvf TextBox/dataset/webnlg2.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZPOAtcDia0S",
        "outputId": "0739abec-4ee5-4195-ccda-13a91ce978c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "webnlg2/\n",
            "webnlg2/valid.src\n",
            "webnlg2/test.tgt\n",
            "webnlg2/train.tgt\n",
            "webnlg2/train.src\n",
            "webnlg2/valid.tgt\n",
            "webnlg2/test.src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd TextBox && python run_textbox.py --model=BART --dataset=webnlg2 --model_path=facebook/bart-base --epochs=5 --metrics=\\[\\'bleu\\',\\'rouge\\'\\]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xP0B-v6iglv",
        "outputId": "05c541dc-d31e-4468-caeb-34d028659c81"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n",
            "01 Feb 15:18    INFO 65 parameters found.\n",
            "================================================================================\n",
            "\n",
            "# General Hyper Parameters: \n",
            "\n",
            "gpu_id: 0\n",
            "use_gpu: True\n",
            "device: cuda\n",
            "seed: 2020\n",
            "reproducibility: True\n",
            "cmd: run_textbox.py --model=BART --dataset=webnlg2 --model_path=facebook/bart-base --epochs=5 --metrics=['bleu','rouge']\n",
            "filename: BART-webnlg2-2023-Feb-01_15-18-23\n",
            "saved_dir: saved/\n",
            "state: INFO\n",
            "wandb: online\n",
            "\n",
            "\n",
            "# Training Hyper Parameters: \n",
            "\n",
            "do_train: True\n",
            "do_valid: True\n",
            "optimizer: adamw\n",
            "adafactor_kwargs: {'lr': 0.001, 'scale_parameter': False, 'relative_step': False, 'warmup_init': False}\n",
            "optimizer_kwargs: {}\n",
            "valid_steps: 1\n",
            "valid_strategy: epoch\n",
            "stopping_steps: 2\n",
            "epochs: 5\n",
            "learning_rate: 3e-05\n",
            "train_batch_size: 20\n",
            "grad_clip: 0.1\n",
            "accumulation_steps: 10\n",
            "disable_tqdm: False\n",
            "resume_training: True\n",
            "\n",
            "\n",
            "# Evaluation Hyper Parameters: \n",
            "\n",
            "do_test: True\n",
            "lower_evaluation: True\n",
            "multiref_strategy: max\n",
            "bleu_max_ngrams: 4\n",
            "bleu_type: sacrebleu\n",
            "smoothing_function: 0\n",
            "corpus_bleu: False\n",
            "rouge_max_ngrams: 2\n",
            "rouge_type: rouge-score\n",
            "meteor_type: pycocoevalcap\n",
            "chrf_type: m-popovic\n",
            "distinct_max_ngrams: 4\n",
            "inter_distinct: True\n",
            "unique_max_ngrams: 4\n",
            "self_bleu_max_ngrams: 4\n",
            "tgt_lang: en\n",
            "metrics: ['bleu', 'rouge']\n",
            "eval_batch_size: 36\n",
            "corpus_meteor: True\n",
            "\n",
            "\n",
            "# Model Hyper Parameters: \n",
            "\n",
            "model: BART\n",
            "model_name: bart\n",
            "model_path: facebook/bart-base\n",
            "config_kwargs: {}\n",
            "tokenizer_kwargs: {'use_fast': True}\n",
            "generation_kwargs: {'num_beams': 5, 'no_repeat_ngram_size': 3, 'early_stopping': True}\n",
            "efficient_kwargs: {}\n",
            "efficient_methods: []\n",
            "efficient_unfreeze_model: False\n",
            "label_smoothing: 0.1\n",
            "\n",
            "\n",
            "# Dataset Hyper Parameters: \n",
            "\n",
            "dataset: webnlg2\n",
            "data_path: dataset/webnlg2\n",
            "tgt_lang: en\n",
            "src_len: 512\n",
            "tgt_len: 128\n",
            "truncate: tail\n",
            "prefix_prompt: Describe the following data: \n",
            "metrics_for_best_model: ['bleu', 'rouge-2', 'meteor']\n",
            "\n",
            "\u001b[33m\n",
            "# Unrecognized Hyper Parameters: \n",
            "\n",
            "tokenizer_add_tokens: []\n",
            "load_type: from_pretrained\n",
            "\u001b[39m\n",
            "================================================================================\n",
            "Downloading (â€¦)lve/main/config.json: 100% 1.72k/1.72k [00:00<00:00, 257kB/s]\u001b[0m\u001b[0m\u001b[0m\n",
            "Downloading (â€¦)olve/main/vocab.json: 100% 899k/899k [00:01<00:00, 798kB/s]\u001b[0m\u001b[0m\u001b[0m\n",
            "Downloading (â€¦)olve/main/merges.txt: 100% 456k/456k [00:01<00:00, 407kB/s]\u001b[0m\u001b[0m\u001b[0m\n",
            "Downloading (â€¦)/main/tokenizer.json: 100% 1.36M/1.36M [00:01<00:00, 1.01MB/s]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 15:18    INFO Pretrain type: pretrain disabled\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtejasvi-chebrolu\u001b[0m (\u001b[33mchebrolu-tejasvi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1msaved/BART-webnlg2-2023-Feb-01_15-18-23/wandb/run-20230201_151846-nb2s1pe1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m2023-Feb-01_15-18-23\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/chebrolu-tejasvi/BART-webnlg2\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/chebrolu-tejasvi/BART-webnlg2/runs/nb2s1pe1\u001b[0m\n",
            "Downloading (â€¦)\"pytorch_model.bin\";: 100% 558M/558M [00:05<00:00, 102MB/s] \u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 15:19    INFO Pretrained_Models(\n",
            "  (model): BartForConditionalGeneration(\n",
            "    (model): BartModel(\n",
            "      (shared): Embedding(50265, 768, padding_idx=1)\n",
            "      (encoder): BartEncoder(\n",
            "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
            "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
            "        (layers): ModuleList(\n",
            "          (0): BartEncoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (activation_fn): GELUActivation()\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (1): BartEncoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (activation_fn): GELUActivation()\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (2): BartEncoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (activation_fn): GELUActivation()\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (3): BartEncoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (activation_fn): GELUActivation()\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (4): BartEncoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (activation_fn): GELUActivation()\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (5): BartEncoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (activation_fn): GELUActivation()\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (decoder): BartDecoder(\n",
            "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
            "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
            "        (layers): ModuleList(\n",
            "          (0): BartDecoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (activation_fn): GELUActivation()\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (encoder_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (1): BartDecoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (activation_fn): GELUActivation()\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (encoder_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (2): BartDecoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (activation_fn): GELUActivation()\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (encoder_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (3): BartDecoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (activation_fn): GELUActivation()\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (encoder_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (4): BartDecoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (activation_fn): GELUActivation()\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (encoder_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (5): BartDecoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (activation_fn): GELUActivation()\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (encoder_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
            "  )\n",
            ")\n",
            "Trainable parameters: 139420416\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 15:19    INFO ====== Start training ======\n",
            "train    1: 100% 178/178 [14:45<00:00,  4.97s/step, loss=2.91]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 15:33    INFO Train epoch  1 [time: 885.52s, loss: 2.91]\n",
            "generating: 100% 47/47 [01:18<00:00,  1.68s/it]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 15:35    INFO Using default tokenizer.\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 15:35    INFO  Validation  1 (best) [time: 85.09s, score: 107.57, <bleu: 48.74>, rouge-1: 77.33, <rouge-2: 58.83>, rouge-l: 70.63]\n",
            "train    2: 100% 178/178 [14:50<00:00,  5.00s/step, loss=2.54]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 15:50    INFO Train epoch  2 [time: 890.13s, loss: 2.54]\n",
            "generating: 100% 47/47 [01:39<00:00,  2.11s/it]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 15:51    INFO Using default tokenizer.\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 15:51    INFO  Validation  2 (best) [time: 105.63s, score: 121.61, <bleu: 57.90>, rouge-1: 82.63, <rouge-2: 63.71>, rouge-l: 74.93]\n",
            "train    3: 100% 178/178 [14:48<00:00,  4.99s/step, loss=2.42]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 16:06    INFO Train epoch  3 [time: 888.32s, loss: 2.42]\n",
            "generating: 100% 47/47 [01:22<00:00,  1.76s/it]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 16:08    INFO Using default tokenizer.\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 16:08    INFO  Validation  3 [time: 89.25s, score: 121.17, <bleu: 56.84>, rouge-1: 82.12, <rouge-2: 64.33>, rouge-l: 75.35]\n",
            "train    4: 100% 178/178 [14:49<00:00,  5.00s/step, loss=2.35]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 16:23    INFO Train epoch  4 [time: 889.93s, loss: 2.35]\n",
            "generating: 100% 47/47 [01:35<00:00,  2.04s/it]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 16:24    INFO Using default tokenizer.\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 16:25    INFO  Validation  4 (best) [time: 102.88s, score: 126.39, <bleu: 60.71>, rouge-1: 83.86, <rouge-2: 65.68>, rouge-l: 76.57]\n",
            "train    5: 100% 178/178 [14:51<00:00,  5.01s/step, loss=2.3]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 16:40    INFO Train epoch  5 [time: 891.56s, loss: 2.30]\n",
            "generating: 100% 47/47 [01:33<00:00,  1.98s/it]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 16:41    INFO Using default tokenizer.\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 16:41    INFO  Validation  5 (best) [time: 99.86s, score: 126.98, <bleu: 61.27>, rouge-1: 84.50, <rouge-2: 65.71>, rouge-l: 76.65]\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 16:41    INFO Soft link created: saved/BART-webnlg2-2023-Feb-01_15-18-23/checkpoint_best -> /content/TextBox/saved/BART-webnlg2-2023-Feb-01_15-18-23/checkpoint_epoch-5\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 16:41    INFO ====== Finished training, best validation result at train epoch 5 ======\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 16:41    INFO Best valid result: score: 126.98, <bleu: 61.27>, rouge-1: 84.50, <rouge-2: 65.71>, rouge-l: 76.65\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 16:41    INFO Loading model structure and parameters from saved/BART-webnlg2-2023-Feb-01_15-18-23/checkpoint_best ...\n",
            "generating: 100% 47/47 [01:33<00:00,  1.99s/it]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 16:43    INFO Using default tokenizer.\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 16:43    INFO Evaluation result:\n",
            " score: 126.98,\n",
            " <bleu: 61.27>,\n",
            " rouge-1: 84.50,\n",
            " <rouge-2: 65.71>,\n",
            " rouge-l: 76.65\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 16:43    INFO Epoch   [time: 100.77s, score: 126.98, <bleu: 61.27>, rouge-1: 84.50, <rouge-2: 65.71>, rouge-l: 76.65]\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss/train â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–‚â–â–‚â–â–â–‚â–â–‚â–â–‚â–‚â–â–â–â–â–â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/bleu â–â–†â–†â–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/rouge-1 â–â–†â–†â–‡â–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/rouge-2 â–â–†â–‡â–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/rouge-l â–â–†â–†â–ˆâ–ˆ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       test/bleu â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test/rouge-1 â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test/rouge-2 â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test/rouge-l â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss/train 2.22412\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/bleu 61.27\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/rouge-1 84.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/rouge-2 65.71\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/rouge-l 76.65\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       test/bleu 61.27\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test/rouge-1 84.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test/rouge-2 65.71\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test/rouge-l 76.65\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33m2023-Feb-01_15-18-23\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/chebrolu-tejasvi/BART-webnlg2/runs/nb2s1pe1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1msaved/BART-webnlg2-2023-Feb-01_15-18-23/wandb/run-20230201_151846-nb2s1pe1/logs\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar zxvf TextBox/dataset/totto.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvxQGOv-in80",
        "outputId": "1abd9067-454e-435b-99c0-3482914f595f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "totto/\n",
            "totto/valid.src\n",
            "totto/test.tgt\n",
            "totto/train.tgt\n",
            "totto/train.src\n",
            "totto/valid.tgt\n",
            "totto/test.src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd TextBox && python run_textbox.py --model=BART --dataset=totto --model_path=facebook/bart-base --epochs=1 --metrics=\\[\\'bleu\\',\\'rouge\\'\\]"
      ],
      "metadata": {
        "id": "54BoCLI276wk",
        "outputId": "24296d0e-c3de-435f-ac93-676878f22797",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n",
            "01 Feb 17:07    INFO 65 parameters found.\n",
            "================================================================================\n",
            "\n",
            "# General Hyper Parameters: \n",
            "\n",
            "gpu_id: 0\n",
            "use_gpu: True\n",
            "device: cuda\n",
            "seed: 2020\n",
            "reproducibility: True\n",
            "cmd: run_textbox.py --model=BART --dataset=totto --model_path=facebook/bart-base --epochs=1 --metrics=['bleu','rouge']\n",
            "filename: BART-totto-2023-Feb-01_17-07-45\n",
            "saved_dir: saved/\n",
            "state: INFO\n",
            "wandb: online\n",
            "\n",
            "\n",
            "# Training Hyper Parameters: \n",
            "\n",
            "do_train: True\n",
            "do_valid: True\n",
            "optimizer: adamw\n",
            "adafactor_kwargs: {'lr': 0.001, 'scale_parameter': False, 'relative_step': False, 'warmup_init': False}\n",
            "optimizer_kwargs: {}\n",
            "valid_steps: 1\n",
            "valid_strategy: epoch\n",
            "stopping_steps: 2\n",
            "epochs: 1\n",
            "learning_rate: 3e-05\n",
            "train_batch_size: 16\n",
            "grad_clip: 0.1\n",
            "accumulation_steps: 12\n",
            "disable_tqdm: False\n",
            "resume_training: True\n",
            "\n",
            "\n",
            "# Evaluation Hyper Parameters: \n",
            "\n",
            "do_test: True\n",
            "lower_evaluation: True\n",
            "multiref_strategy: max\n",
            "bleu_max_ngrams: 4\n",
            "bleu_type: sacrebleu\n",
            "smoothing_function: 0\n",
            "corpus_bleu: False\n",
            "rouge_max_ngrams: 2\n",
            "rouge_type: rouge-score\n",
            "meteor_type: pycocoevalcap\n",
            "chrf_type: m-popovic\n",
            "distinct_max_ngrams: 4\n",
            "inter_distinct: True\n",
            "unique_max_ngrams: 4\n",
            "self_bleu_max_ngrams: 4\n",
            "tgt_lang: en\n",
            "metrics: ['bleu', 'rouge']\n",
            "eval_batch_size: 32\n",
            "corpus_meteor: True\n",
            "\n",
            "\n",
            "# Model Hyper Parameters: \n",
            "\n",
            "model: BART\n",
            "model_name: bart\n",
            "model_path: facebook/bart-base\n",
            "config_kwargs: {}\n",
            "tokenizer_kwargs: {'use_fast': True}\n",
            "generation_kwargs: {'num_beams': 5, 'no_repeat_ngram_size': 3, 'early_stopping': True}\n",
            "efficient_kwargs: {}\n",
            "efficient_methods: []\n",
            "efficient_unfreeze_model: False\n",
            "label_smoothing: 0.1\n",
            "\n",
            "\n",
            "# Dataset Hyper Parameters: \n",
            "\n",
            "dataset: totto\n",
            "data_path: dataset/totto\n",
            "tgt_lang: en\n",
            "src_len: 256\n",
            "tgt_len: 64\n",
            "truncate: tail\n",
            "metrics_for_best_model: ['bleu', 'rouge-2', 'meteor']\n",
            "prefix_prompt: Describe the following data: \n",
            "\n",
            "\u001b[33m\n",
            "# Unrecognized Hyper Parameters: \n",
            "\n",
            "tokenizer_add_tokens: []\n",
            "load_type: from_pretrained\n",
            "\u001b[39m\n",
            "================================================================================\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 17:07    INFO Pretrain type: pretrain disabled\n",
            "\u001b[0m\u001b[0m\u001b[0mToken indices sequence length is longer than the specified maximum sequence length for this model (1200 > 1024). Running this sequence through the model will result in indexing errors\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtejasvi-chebrolu\u001b[0m (\u001b[33mchebrolu-tejasvi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1msaved/BART-totto-2023-Feb-01_17-07-45/wandb/run-20230201_170812-59mxpuhv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m2023-Feb-01_17-07-45\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/chebrolu-tejasvi/BART-totto\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/chebrolu-tejasvi/BART-totto/runs/59mxpuhv\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 17:08    INFO Pretrained_Models(\n",
            "  (model): BartForConditionalGeneration(\n",
            "    (model): BartModel(\n",
            "      (shared): Embedding(50265, 768, padding_idx=1)\n",
            "      (encoder): BartEncoder(\n",
            "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
            "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
            "        (layers): ModuleList(\n",
            "          (0): BartEncoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (activation_fn): GELUActivation()\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (1): BartEncoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (activation_fn): GELUActivation()\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (2): BartEncoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (activation_fn): GELUActivation()\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (3): BartEncoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (activation_fn): GELUActivation()\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (4): BartEncoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (activation_fn): GELUActivation()\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (5): BartEncoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (activation_fn): GELUActivation()\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (decoder): BartDecoder(\n",
            "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
            "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
            "        (layers): ModuleList(\n",
            "          (0): BartDecoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (activation_fn): GELUActivation()\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (encoder_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (1): BartDecoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (activation_fn): GELUActivation()\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (encoder_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (2): BartDecoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (activation_fn): GELUActivation()\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (encoder_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (3): BartDecoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (activation_fn): GELUActivation()\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (encoder_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (4): BartDecoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (activation_fn): GELUActivation()\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (encoder_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (5): BartDecoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (activation_fn): GELUActivation()\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (encoder_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
            "  )\n",
            ")\n",
            "Trainable parameters: 139420416\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 17:08    INFO ====== Start training ======\n",
            "train    1: 100% 629/629 [42:07<00:00,  4.02s/step, loss=2.88]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 17:50    INFO Train epoch  1 [time: 2527.01s, loss: 2.88]\n",
            "generating: 100% 241/241 [07:04<00:00,  1.76s/it]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 17:57    INFO Using default tokenizer.\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 17:57    INFO  Validation  1 (best) [time: 448.39s, score: 88.44, <bleu: 40.00>, rouge-1: 70.34, <rouge-2: 48.44>, rouge-l: 61.30]\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 17:58    INFO Soft link created: saved/BART-totto-2023-Feb-01_17-07-45/checkpoint_best -> /content/TextBox/saved/BART-totto-2023-Feb-01_17-07-45/checkpoint_epoch-1\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 17:58    INFO ====== Finished training, best validation result at train epoch 1 ======\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 17:58    INFO Best valid result: score: 88.44, <bleu: 40.00>, rouge-1: 70.34, <rouge-2: 48.44>, rouge-l: 61.30\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 17:58    INFO Loading model structure and parameters from saved/BART-totto-2023-Feb-01_17-07-45/checkpoint_best ...\n",
            "generating: 100% 241/241 [07:05<00:00,  1.76s/it]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 18:05    INFO Using default tokenizer.\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 18:05    INFO Evaluation result:\n",
            " score: 88.44,\n",
            " <bleu: 40.00>,\n",
            " rouge-1: 70.34,\n",
            " <rouge-2: 48.44>,\n",
            " rouge-l: 61.30\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 18:05    INFO Epoch   [time: 449.92s, score: 88.44, <bleu: 40.00>, rouge-1: 70.34, <rouge-2: 48.44>, rouge-l: 61.30]\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss/train â–ˆâ–…â–‡â–†â–ƒâ–†â–†â–…â–…â–„â–„â–ƒâ–„â–ƒâ–…â–â–ˆâ–‚â–‚â–„â–‚â–ƒâ–‚â–„â–ƒâ–„â–ƒâ–„â–„â–ƒâ–„â–â–â–„â–„â–„â–â–â–„â–‚\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/bleu â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/rouge-1 â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/rouge-2 â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/rouge-l â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       test/bleu â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test/rouge-1 â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test/rouge-2 â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test/rouge-l â–\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss/train 2.72102\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    metrics/bleu 40.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/rouge-1 70.34\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/rouge-2 48.44\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: metrics/rouge-l 61.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       test/bleu 40.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test/rouge-1 70.34\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test/rouge-2 48.44\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test/rouge-l 61.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33m2023-Feb-01_17-07-45\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/chebrolu-tejasvi/BART-totto/runs/59mxpuhv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 2 media file(s), 4 artifact file(s) and 0 other file(s)\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1msaved/BART-totto-2023-Feb-01_17-07-45/wandb/run-20230201_170812-59mxpuhv/logs\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd TextBox && python run_textbox.py --model=BART --dataset=webnlg --model_path=facebook/bart-base --epochs=10 --metrics=\\[\\'bleu\\',\\'rouge\\'\\]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHDh_2iGwOv5",
        "outputId": "6559480e-d269-496f-fa8a-b36584f82970"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n",
            "01 Feb 13:19    INFO 66 parameters found.\n",
            "================================================================================\n",
            "\n",
            "# General Hyper Parameters: \n",
            "\n",
            "gpu_id: 0\n",
            "use_gpu: True\n",
            "device: cuda\n",
            "seed: 2020\n",
            "reproducibility: True\n",
            "cmd: run_textbox.py --model=BART --dataset=webnlg --model_path=facebook/bart-base --epochs=10 --metrics=['bleu','rouge']\n",
            "filename: BART-webnlg-2023-Feb-01_13-19-50\n",
            "saved_dir: saved/\n",
            "state: INFO\n",
            "wandb: online\n",
            "\n",
            "\n",
            "# Training Hyper Parameters: \n",
            "\n",
            "do_train: True\n",
            "do_valid: True\n",
            "optimizer: adamw\n",
            "adafactor_kwargs: {'lr': 0.001, 'scale_parameter': False, 'relative_step': False, 'warmup_init': False}\n",
            "optimizer_kwargs: {}\n",
            "valid_steps: 1\n",
            "valid_strategy: epoch\n",
            "stopping_steps: 2\n",
            "epochs: 10\n",
            "learning_rate: 3e-05\n",
            "train_batch_size: 20\n",
            "grad_clip: 0.1\n",
            "accumulation_steps: 10\n",
            "disable_tqdm: False\n",
            "resume_training: True\n",
            "\n",
            "\n",
            "# Evaluation Hyper Parameters: \n",
            "\n",
            "do_test: True\n",
            "lower_evaluation: True\n",
            "multiref_strategy: max\n",
            "bleu_max_ngrams: 4\n",
            "bleu_type: mt-eval\n",
            "smoothing_function: 0\n",
            "corpus_bleu: False\n",
            "rouge_max_ngrams: 2\n",
            "rouge_type: pycocoevalcap\n",
            "meteor_type: pycocoevalcap\n",
            "chrf_type: m-popovic\n",
            "distinct_max_ngrams: 4\n",
            "inter_distinct: True\n",
            "unique_max_ngrams: 4\n",
            "self_bleu_max_ngrams: 4\n",
            "tgt_lang: en\n",
            "metrics: ['bleu', 'rouge']\n",
            "eval_batch_size: 36\n",
            "corpus_meteor: True\n",
            "\n",
            "\n",
            "# Model Hyper Parameters: \n",
            "\n",
            "model: BART\n",
            "model_name: bart\n",
            "model_path: facebook/bart-base\n",
            "config_kwargs: {}\n",
            "tokenizer_kwargs: {'use_fast': True}\n",
            "generation_kwargs: {'num_beams': 5, 'no_repeat_ngram_size': 3, 'early_stopping': True}\n",
            "efficient_kwargs: {}\n",
            "efficient_methods: []\n",
            "efficient_unfreeze_model: False\n",
            "label_smoothing: 0.1\n",
            "\n",
            "\n",
            "# Dataset Hyper Parameters: \n",
            "\n",
            "dataset: webnlg\n",
            "data_path: dataset/webnlg\n",
            "tgt_lang: en\n",
            "src_len: 512\n",
            "tgt_len: 128\n",
            "truncate: tail\n",
            "remove_punc: True\n",
            "metrics_for_best_model: ['bleu', 'meteor', 'rouge-l']\n",
            "prefix_prompt: Describe the following data: \n",
            "\n",
            "\u001b[33m\n",
            "# Unrecognized Hyper Parameters: \n",
            "\n",
            "tokenizer_add_tokens: []\n",
            "load_type: from_pretrained\n",
            "\u001b[39m\n",
            "================================================================================\n",
            "Downloading (â€¦)lve/main/config.json: 100% 1.72k/1.72k [00:00<00:00, 288kB/s]\u001b[0m\u001b[0m\u001b[0m\n",
            "Downloading (â€¦)olve/main/vocab.json: 100% 899k/899k [00:01<00:00, 672kB/s]\u001b[0m\u001b[0m\u001b[0m\n",
            "Downloading (â€¦)olve/main/merges.txt: 100% 456k/456k [00:01<00:00, 413kB/s]\u001b[0m\u001b[0m\u001b[0m\n",
            "Downloading (â€¦)/main/tokenizer.json: 100% 1.36M/1.36M [00:01<00:00, 994kB/s]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 13:20    INFO Pretrain type: pretrain disabled\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtejasvi-chebrolu\u001b[0m (\u001b[33mchebrolu-tejasvi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.9\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1msaved/BART-webnlg-2023-Feb-01_13-19-50/wandb/run-20230201_132013-hwz9crbx\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m2023-Feb-01_13-19-50\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/chebrolu-tejasvi/BART-webnlg\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/chebrolu-tejasvi/BART-webnlg/runs/hwz9crbx\u001b[0m\n",
            "Downloading (â€¦)\"pytorch_model.bin\";: 100% 558M/558M [00:40<00:00, 13.8MB/s]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 13:21    INFO Pretrained_Models(\n",
            "  (model): BartForConditionalGeneration(\n",
            "    (model): BartModel(\n",
            "      (shared): Embedding(50265, 768, padding_idx=1)\n",
            "      (encoder): BartEncoder(\n",
            "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
            "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
            "        (layers): ModuleList(\n",
            "          (0): BartEncoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (activation_fn): GELUActivation()\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (1): BartEncoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (activation_fn): GELUActivation()\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (2): BartEncoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (activation_fn): GELUActivation()\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (3): BartEncoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (activation_fn): GELUActivation()\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (4): BartEncoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (activation_fn): GELUActivation()\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (5): BartEncoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (activation_fn): GELUActivation()\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (decoder): BartDecoder(\n",
            "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
            "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
            "        (layers): ModuleList(\n",
            "          (0): BartDecoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (activation_fn): GELUActivation()\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (encoder_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (1): BartDecoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (activation_fn): GELUActivation()\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (encoder_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (2): BartDecoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (activation_fn): GELUActivation()\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (encoder_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (3): BartDecoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (activation_fn): GELUActivation()\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (encoder_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (4): BartDecoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (activation_fn): GELUActivation()\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (encoder_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "          (5): BartDecoderLayer(\n",
            "            (self_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (activation_fn): GELUActivation()\n",
            "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (encoder_attn): BartAttention(\n",
            "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          )\n",
            "        )\n",
            "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
            "  )\n",
            ")\n",
            "Trainable parameters: 139420416\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 13:21    INFO ====== Start training ======\n",
            "train    1: 100% 172/172 [12:45<00:00,  4.45s/step, loss=2.95]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 13:33    INFO Train epoch  1 [time: 765.99s, loss: 2.95]\n",
            "generating: 100% 45/45 [01:25<00:00,  1.91s/it]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 13:35    INFO  Validation  1 (best) [time: 88.13s, score: 117.70, <bleu: 51.57>, <rouge-l: 66.13>]\n",
            "train    2: 100% 172/172 [12:48<00:00,  4.47s/step, loss=2.56]\u001b[0m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0m\u001b[0m01 Feb 13:48    INFO Train epoch  2 [time: 768.43s, loss: 2.56]\n",
            "generating:  53% 24/45 [00:31<00:42,  2.04s/it]\u001b[0m\u001b[0m\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pm9L1OckyHuu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}